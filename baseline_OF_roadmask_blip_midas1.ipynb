{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54329b51-f005-4b98-b21f-148b3285c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ed7054-9cfc-4be2-89f0-c69db22ea89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = open(\"./annotations_public.pkl\", 'rb')\n",
    "annotations = pickle.load(annotation_file)#Load annotations\n",
    "annotation_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333ef87-a3e4-4cdc-be32-3af0c630240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiDas model for depth calculation in frames\n",
    "model_type = \"DPT_Large\"\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "\n",
    "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
    "    transform = midas_transforms.dpt_transform\n",
    "else:\n",
    "    transform = midas_transforms.small_transform\n",
    "\n",
    "# params for corner detection \n",
    "feature_params = dict(maxCorners=100, \n",
    "                      qualityLevel=0.3, \n",
    "                      minDistance=7, \n",
    "                      blockSize=7) \n",
    "\n",
    "# Parameters for lucas kanade optical flow \n",
    "lk_params = dict(winSize=(15, 15), \n",
    "                 maxLevel=2, \n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, \n",
    "                           10, 0.03)) \n",
    "\n",
    "hazard_summary = {}\n",
    "\n",
    "# Create some random colors \n",
    "color = np.random.randint(0, 255, (100, 3)) \n",
    "video_root = './COOOL_Benchmark/processed_videos/'\n",
    "output_dir = \"./COOOL_Benchmark/processed_videos_midas/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "video_num = 0\n",
    "\n",
    "def is_object_far(x1, y1, x2, y2, frame_image):\n",
    "    input_batch = transform(frame_image).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "          prediction = midas(input_batch)\n",
    "    \n",
    "          prediction = torch.nn.functional.interpolate(\n",
    "              prediction.unsqueeze(1),\n",
    "              size=frame_image.shape[:2],\n",
    "              mode=\"bicubic\",\n",
    "              align_corners=False,\n",
    "          ).squeeze()\n",
    "\n",
    "    output = prediction.cpu().numpy()\n",
    "    object_color = output[int(y1+(abs(y2-y1)/2)), int(x1+(abs(x2-x1)/2))]\n",
    "    return object_color\n",
    "\n",
    "\n",
    "def filter_objects_by_midas(det_far):    \n",
    "    obj_far = []\n",
    "    for track_id, color in det_far.items():\n",
    "        obj_far.append((track_id, color))\n",
    "    obj_far.sort(key=lambda x: x[-1], reverse=True) #in descending order\n",
    "    filtered_dist_color = det_far.copy()\n",
    "    \n",
    "    if len(obj_far) == 2: # if there are two objects, exclude with caution\n",
    "        _, cent_0 = obj_far[0]\n",
    "        _, cent_1 = obj_far[1]\n",
    "        dist_color = abs(cent_0 - cent_1)\n",
    "        if dist_color > 6:\n",
    "            filtered_dist_color = {}\n",
    "            track, color = obj_far[0]\n",
    "            filtered_dist_color[track] = color\n",
    "            \n",
    "    elif len(obj_far) > 2:# if there are more than two objects, exclude objects more freely :)\n",
    "        filter_average = {}\n",
    "        color_list = [color_x for _, color_x, in obj_far]\n",
    "        if color_list:\n",
    "            avg_color_list = sum(color_list) / len(color_list)\n",
    "        else:\n",
    "            avg_color_list = 0  # No objects to compare\n",
    "        for track_id_obj, color in obj_far:\n",
    "            if color > avg_color_list:\n",
    "                filter_average[track_id_obj] = color\n",
    "        if len(filter_average) == 1 and len(obj_far)>1 and abs(obj_far[0][1] - obj_far[1][1]) < 6: #to address horse video\n",
    "            filter_average[obj_far[1][0]] = obj_far[1][1] # to avoid filtering out all objects\n",
    "                    \n",
    "        filtered_dist_color = filter_average\n",
    "    return filtered_dist_color \n",
    "\n",
    "\n",
    "def retain_first_and_get_unique_ids(def_far_all, num_id):\n",
    "    # first_id_per_frame = {}\n",
    "    unique_ids = set()\n",
    "    unique_hazards = set()\n",
    "    object_values = {}  # To track values of objects across all frames\n",
    "    brightest_frame_per_object = {}  # Track the brightest frame for each object\n",
    "\n",
    "    for frame, objects in def_far_all.items():\n",
    "        objects = dict(sorted(objects.items(), key=lambda item: item[1], reverse=True))\n",
    "        bright_objects = []\n",
    "        # Check for objects with brightness value > 15\n",
    "        if num_id <= 4:\n",
    "            bright_objects.extend([obj_id for obj_id, value in objects.items() if value > 15])\n",
    "            unique_hazards.update(bright_objects)\n",
    "\n",
    "            # Update the brightest frame for bright objects\n",
    "            for obj_id in bright_objects:\n",
    "                if obj_id not in brightest_frame_per_object or objects[obj_id] > object_values.get(obj_id, 0):\n",
    "                    brightest_frame_per_object[obj_id] = (frame, objects[obj_id])\n",
    "                    object_values[obj_id] = objects[obj_id]\n",
    "\n",
    "\n",
    "        if objects and len(unique_hazards) == 0:  #4\n",
    "            first_object_id = list(objects.keys())[0]\n",
    "            unique_ids.add(first_object_id)\n",
    "\n",
    "           # Track the brightness value and brightest frame for fallback objects\n",
    "            if first_object_id not in object_values or objects[first_object_id] > object_values.get(first_object_id, 0):\n",
    "                brightest_frame_per_object[first_object_id] = (frame, objects[first_object_id])\n",
    "                object_values[first_object_id] = objects[first_object_id]\n",
    "\n",
    "        \n",
    "    # If num_id > 3, remove the object with the least value in `unique_ids`\n",
    "    if num_id>4 and len(unique_ids)>3:\n",
    "        sorted_objects = sorted(unique_ids, key=lambda obj_id: object_values.get(obj_id, float('inf')))\n",
    "    \n",
    "        if len(sorted_objects) > 1:  # Ensure there are at least two objects to compare\n",
    "            min_value_object = sorted_objects[0]\n",
    "            second_min_value = object_values.get(sorted_objects[1], float('inf'))\n",
    "            \n",
    "            # Check the difference between the smallest and the second smallest value\n",
    "            if second_min_value - object_values[min_value_object] > 1:\n",
    "                unique_ids.remove(min_value_object)\n",
    "\n",
    "    return unique_ids, unique_hazards, brightest_frame_per_object\n",
    "\n",
    "\n",
    "with open(\"results_md_blip_final.csv\", 'w') as results_file:\n",
    "    results_file.write(\"ID,Driver_State_Changed\")\n",
    "    for i in range(23):\n",
    "        results_file.write(f\",Hazard_Track_{i},Hazard_Name_{i}\")\n",
    "    results_file.write(\"\\n\")\n",
    "        \n",
    "    for video in sorted(list(annotations.keys())):\n",
    "        video_num += 1 \n",
    "        print(\"video_num:\", video_num)\n",
    "        if video_num > 0:\n",
    "            video_stream = cv2.VideoCapture(os.path.join(video_root, video+'.mp4'))\n",
    "            assert video_stream.isOpened()\n",
    "            \n",
    "            while video_stream.isOpened():\n",
    "                ret, old_frame = video_stream.read() \n",
    "                old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "                if np.all(old_gray == 0):\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "            p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params) \n",
    "            \n",
    "            # Create a mask image for drawing purposes \n",
    "            mask = np.zeros_like(old_frame) \n",
    "            \n",
    "            frame = 0\n",
    "            slope_history = []  # Store slopes for every 5 frames\n",
    "            track_id_lifecycle = {} \n",
    "            threshold = 4  # Define a threshold for fast slope change\n",
    "            driver_state_flag = False\n",
    "            \n",
    "            video_stream = cv2.VideoCapture(os.path.join(video_root, video+'.mp4'))\n",
    "            # Get video properties\n",
    "            fps = int(video_stream.get(cv2.CAP_PROP_FPS))\n",
    "            width = int(video_stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(video_stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for output video\n",
    "            output_video_path = os.path.join(output_dir, f\"{video}_midas.mp4\")\n",
    "            out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "            def_far_all = {}\n",
    "            \n",
    "            while video_stream.isOpened():\n",
    "                video_frame = f'{video}_{frame}'\n",
    "                ret, frame_image = video_stream.read()\n",
    "                if ret == False: #False means end of video or error\n",
    "                    assert frame == len(annotations[video].keys()) #End of the video must be final frame\n",
    "                    break\n",
    "        #########################################################################################             \n",
    "                #Gather BBoxes from annotations\n",
    "                ###########\n",
    "                bboxes = {}\n",
    "                centroids = []\n",
    "                chips = {}\n",
    "                track_ids = []\n",
    "                det_far = {}\n",
    "                for ann_type in ['challenge_object']:\n",
    "                    for i in range(len(annotations[video][frame][ann_type])):\n",
    "                        x1, y1, x2, y2 = annotations[video][frame][ann_type][i]['bbox']\n",
    "                        track_id = annotations[video][frame][ann_type][i]['track_id']\n",
    "        \n",
    "                        # Update lifecycle of the track_id\n",
    "                        if track_id not in track_id_lifecycle:\n",
    "                            track_id_lifecycle[track_id] = {'first_frame': frame, 'last_frame': frame}\n",
    "                        else:\n",
    "                            track_id_lifecycle[track_id]['last_frame'] = frame\n",
    "        \n",
    "                        # if track_id not in bboxes:\n",
    "                        bboxes[track_id] = {'frame': frame, 'bboxes': [x1, y1, x2, y2]}\n",
    "                        chips[track_id]= {'frame': frame, 'chip': frame_image[int(y1):int(y2), int(x1):int(x2)]}\n",
    "                        color = is_object_far(x1, y1, x2, y2, frame_image)\n",
    "                        det_far[track_id] = color.item()\n",
    "                ##################\n",
    "            ##################    \n",
    "                if frame not in def_far_all:\n",
    "                    def_far_all[frame] = filter_objects_by_midas(det_far)\n",
    "                else:\n",
    "                    def_far_all[frame].update(filter_objects_by_midas(det_far))\n",
    "             #######################  \n",
    "                if frame==0:\n",
    "                   frame +=1\n",
    "                   continue #We can't make a prediction of state change w/o knowing the previous state\n",
    "                    \n",
    "          #########################################################################################                \n",
    "                ###Driver state change detection\n",
    "                frame_gray = cv2.cvtColor(frame_image, cv2.COLOR_BGR2GRAY) \n",
    "        \n",
    "                # calculate optical flow \n",
    "                p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params) \n",
    "            \n",
    "                # Select good points \n",
    "                good_new = p1[st == 1] \n",
    "                good_old = p0[st == 1] \n",
    "                \n",
    "                # Calculate motion vectors and slopes\n",
    "                motion_vectors = []\n",
    "                # Calculate slopes for the current frame\n",
    "                slopes = []\n",
    "                for i, (new, old) in enumerate(zip(good_new, good_old)): \n",
    "                    a, b = new.ravel() \n",
    "                    c, d = old.ravel() \n",
    "                    \n",
    "                    slope = (b - d) / ((a - c) + 1e-100)\n",
    "                    slopes.append(slope)\n",
    "            \n",
    "                avg_slope_change = 0\n",
    "                stop = \"\"\n",
    "\n",
    "                # Update slope history every 5 frames                \n",
    "                if frame % 5 == 0:\n",
    "                    if slope_history:\n",
    "                        previous_slopes = slope_history[-1]\n",
    "                        slope_changes = [abs(s - ps) for s, ps in zip(slopes, previous_slopes)]\n",
    "                        avg_slope_change = np.mean(slope_changes) \n",
    "                        if avg_slope_change < threshold and driver_state_flag != True:\n",
    "                            driver_state_flag = True\n",
    "                            print(f\"Frame {frame} labeled as True\")           \n",
    "                    slope_history.append(slopes)  # Update the history\n",
    "    \n",
    "                cv2.putText(frame_image, str(driver_state_flag)+stop, (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "                row_data = {\"ID\": video_frame, \"Driver_State_Changed\": driver_state_flag}\n",
    "                for i in range(23):\n",
    "                    row_data[f\"Hazard_Track_{i}\"] = \"\"\n",
    "                    row_data[f\"Hazard_Name_{i}\"] = \"\"\n",
    "                        \n",
    "                out.write(frame_image)\n",
    "                results_file.write(\n",
    "                    f\"{row_data['ID']},{row_data['Driver_State_Changed']}\" +\n",
    "                    \"\".join([f\",{row_data[f'Hazard_Track_{i}']},{row_data[f'Hazard_Name_{i}']}\" for i in range(23)]) +\n",
    "                    \"\\n\"\n",
    "                )\n",
    "                                           \n",
    "                frame +=1\n",
    "           \n",
    "            video_stream.release()\n",
    "            out.release()\n",
    "    ###################################\n",
    "        ##MiDas: to exclude very far objects\n",
    "            num_id = len(track_id_lifecycle)\n",
    "            # print(\"num_id:\", num_id)\n",
    "            frame_midas = 0        \n",
    "            unique_ids, unique_hazards, brightest_frame_per_object = retain_first_and_get_unique_ids(def_far_all, num_id)\n",
    "            # print(\"unique_ids:\", unique_ids)\n",
    "            # print(\"unique_hazards:\", unique_hazards)\n",
    "            if len(unique_hazards) > 0:\n",
    "                unique_ids = unique_hazards\n",
    "            # print(\"unique_ids:\", unique_ids)\n",
    "            # video_stream.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset to the first frame\n",
    "            video_stream_midas = cv2.VideoCapture(os.path.join(output_dir, f\"{video}_midas.mp4\"))\n",
    "            out_midas = cv2.VideoWriter(\n",
    "                os.path.join(output_dir, f\"{video}_midas_hazard_v1.mp4\"),\n",
    "                cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                int(video_stream_midas.get(cv2.CAP_PROP_FPS)),\n",
    "                (int(video_stream_midas.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video_stream_midas.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "            )\n",
    "            hazard_midas = {}\n",
    "            while video_stream_midas.isOpened():\n",
    "                ret, frame_image_midas = video_stream_midas.read()\n",
    "                if ret == False: #False means end of video or error\n",
    "                    assert frame_midas == len(annotations[video].keys())-1 #End of the video must be final frame\n",
    "                    break\n",
    "                    \n",
    "                for ann_type in ['challenge_object']:\n",
    "                    for i in range(len(annotations[video][frame_midas][ann_type])):\n",
    "                        x1, y1, x2, y2 = annotations[video][frame_midas][ann_type][i]['bbox']\n",
    "                        track_id = annotations[video][frame_midas][ann_type][i]['track_id']\n",
    "                        # print(\"track_id:\", track_id)\n",
    "                        if str(track_id) in unique_ids:  # Only process unique IDs\n",
    "                            # if frame_midas not in hazard_midas:\n",
    "                            #     hazard_midas[frame_midas] = [] \n",
    "                            # hazard_midas[frame_midas].append(track_id)\n",
    "                            # print(\"hazard_midas:\", hazard_midas)\n",
    "                            cv2.rectangle(frame_image_midas, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                            cv2.putText(frame_image_midas, f\"ID: {track_id}\", (int(x1), int(y1) - 10),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "                            \n",
    "                out_midas.write(frame_image_midas)\n",
    "                frame_midas += 1\n",
    "                \n",
    "            unique_ids_with_brightest_frame = {obj_id: brightest_frame_per_object[obj_id][0] for obj_id in unique_ids}\n",
    "            # Load the hazard_midas dictionary\n",
    "            with open(f\"./unique_ids/unique_ids_{video}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(unique_ids_with_brightest_frame, f)\n",
    "    \n",
    "            video_stream_midas.release()\n",
    "            out_midas.release()\n",
    "        ############################\n",
    "            \n",
    "results_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
