{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50582a87-8a75-4aa8-932f-1b8d58df1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from clip_interrogator import Config, Interrogator\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "annotations_path = 'annotations_public.pkl'\n",
    "video_root = './COOOL_Benchmark/processed_videos/'\n",
    "output_video_dir = 'COOOL_Benchmark/road_mask_Dr_sh/'\n",
    "os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "assert os.path.exists(annotations_path), f\"Annotations file not found at {annotations_path}\"\n",
    "with open(annotations_path, 'rb') as f:\n",
    "    annotations = pickle.load(f)\n",
    "\n",
    "for vid in annotations.keys():\n",
    "    video_path = os.path.join(video_root, vid + \".mp4\")\n",
    "    assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n",
    "print(\"All videos verified.\")\n",
    "\n",
    "ci = Interrogator(Config(clip_model_name=\"ViT-B-32/openai\"))\n",
    "\n",
    "def clean_caption(caption):\n",
    "    unwanted_terms = [\n",
    "        'ape', 'xqcow', 'emote', 'bad vhs quality', 'damaged webcam image',\n",
    "        'aliased', 'orb', 'round-cropped', 'zoom shot', 'full figure',\n",
    "        '200mm', 'dehazed image', 'zoomed', 'circle beard', 'clenched fist'\n",
    "    ]\n",
    "    for term in unwanted_terms:\n",
    "        caption = caption.replace(term, '')\n",
    "    caption = caption.replace(',', ' ')\n",
    "    caption = ' '.join(caption.split())\n",
    "    caption = caption[:35]\n",
    "    return caption.strip()\n",
    "\n",
    "seg_model_name = \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\"\n",
    "image_processor_road = SegformerImageProcessor.from_pretrained(seg_model_name)\n",
    "seg_model = SegformerForSemanticSegmentation.from_pretrained(seg_model_name).to(device)\n",
    "seg_model.eval()\n",
    "\n",
    "def get_road_mask(frame_image):\n",
    "    image_rgb = cv2.cvtColor(frame_image, cv2.COLOR_BGR2RGB)\n",
    "    inputs = image_processor_road(images=image_rgb, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = seg_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    h, w = frame_image.shape[:2]\n",
    "    upsampled_logits = F.interpolate(logits, size=(h, w), mode='bilinear', align_corners=False)\n",
    "    pred = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n",
    "    # class 0 is road\n",
    "    road_mask = (pred == 0).astype(np.uint8) * 255\n",
    "    return road_mask\n",
    "\n",
    "text_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "reference_words = [\"animal\", \"human\"]\n",
    "ref_embeddings = text_model.encode(reference_words)\n",
    "similarity_threshold = 0\n",
    "\n",
    "video_road_objects = {}  \n",
    "\n",
    "for video in sorted(annotations.keys()):\n",
    "    video_path = os.path.join(video_root, video + \".mp4\")\n",
    "    video_stream = cv2.VideoCapture(video_path)\n",
    "    if not video_stream.isOpened():\n",
    "        print(f\"Failed to open video: {video}\")\n",
    "        continue\n",
    "\n",
    "    frame_width = int(video_stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video_stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = video_stream.get(cv2.CAP_PROP_FPS)\n",
    "    output_video_path = os.path.join(output_video_dir, f\"{video}_processed.mp4\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    video_road_objects[video] = set()\n",
    "    captioned_tracks = {}\n",
    "    frame_num = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame_image = video_stream.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        print(f'Processing {video}, frame {frame_num}')\n",
    "\n",
    "        road_mask = get_road_mask(frame_image)\n",
    "        img_h, img_w = frame_image.shape[:2]\n",
    "\n",
    "        if frame_num in annotations[video]:\n",
    "            for obj in annotations[video][frame_num]['challenge_object']:\n",
    "                track_id = obj.get('track_id', None)\n",
    "                if track_id is None:\n",
    "                    continue\n",
    "\n",
    "                x1, y1, x2, y2 = obj['bbox']\n",
    "                x1_clamp = max(0, int(x1))\n",
    "                y1_clamp = max(0, int(y1))\n",
    "                x2_clamp = min(img_w, int(x2))\n",
    "                y2_clamp = min(img_h, int(y2))\n",
    "\n",
    "                if x2_clamp <= x1_clamp or y2_clamp <= y1_clamp:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                if track_id not in captioned_tracks:\n",
    "                    chip = frame_image[y1_clamp:y2_clamp, x1_clamp:x2_clamp]\n",
    "                    chip_rgb = cv2.cvtColor(chip, cv2.COLOR_BGR2RGB)\n",
    "                    chip_pil = Image.fromarray(chip_rgb)\n",
    "                    obj_caption = ci.interrogate(chip_pil)\n",
    "                    obj_caption = clean_caption(obj_caption)\n",
    "                    captioned_tracks[track_id] = obj_caption\n",
    "                else:\n",
    "                    obj_caption = captioned_tracks[track_id]\n",
    "\n",
    "                cls_embedding = text_model.encode([obj_caption])[0]\n",
    "                sim_animal = cosine_similarity([cls_embedding], [ref_embeddings[0]])[0][0]\n",
    "                sim_person = cosine_similarity([cls_embedding], [ref_embeddings[1]])[0][0]\n",
    "\n",
    "                if sim_animal > similarity_threshold or sim_person > similarity_threshold:\n",
    "                    cx_int = int(x1 + (x2 - x1) / 2)\n",
    "                    cy_int = int(y1 + (y2 - y1) / 2)\n",
    "                    on_road = False\n",
    "                    if 0 <= cx_int < img_w and 0 <= cy_int < img_h:\n",
    "                        on_road = (road_mask[cy_int, cx_int] == 255)\n",
    "\n",
    "                    if on_road:\n",
    "                        video_road_objects[video].add(track_id)\n",
    "                        print(f\"Video: {video}, Track ID: {track_id}\")\n",
    "\n",
    "                        cv2.rectangle(frame_image, (x1_clamp, y1_clamp), (x2_clamp, y2_clamp), (0, 255, 0), 2)\n",
    "                        cv2.putText(frame_image, f\"ID:{sim_animal.round(2)}\", (x1_clamp, y2_clamp + 50),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "                        cv2.putText(frame_image, f\"ID:{sim_person.round(2)}\", (x1_clamp, y2_clamp + 35),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "                        cv2.putText(frame_image, f\"ID: {track_id}\", (x1_clamp, y1_clamp - 10),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                        cv2.putText(frame_image, obj_caption, (x1_clamp, y2_clamp + 20),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        out_video.write(frame_image)\n",
    "        frame_num += 1\n",
    "\n",
    "    video_stream.release()\n",
    "    out_video.release()\n",
    "\n",
    "with open(\"road_objects.csv\", 'w') as f:\n",
    "    f.write(\"video,track_id\\n\")\n",
    "    for vid, tid_set in video_road_objects.items():\n",
    "        for tid in tid_set:\n",
    "            f.write(f\"{vid},{tid}\\n\")\n",
    "\n",
    "print(\"Processing complete. Results saved to road_objects.csv and processed videos in 'Processed_Videos/' directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
