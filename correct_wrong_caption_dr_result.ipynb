{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c25f5b9-61c4-4db3-8c2e-b821f863e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "from clip_interrogator import Config, Interrogator\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2abadeee-4b70-4131-9c1f-7a1f24921710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hazard(object_cor, frame):\n",
    "    is_it_hazard = False\n",
    "    caption = \"\"\n",
    "    x1, y1, x2, y2 = object_cor\n",
    "    if x1 < 0:\n",
    "       x1 = 0\n",
    "    if x2 < 0:\n",
    "       x2 = 0\n",
    "    if y1 < 0:\n",
    "       y1 = 0\n",
    "    if y2 < 0:\n",
    "       y2 = 0\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "    # Check if the coordinates are within bounds\n",
    "    if (y1 - 20 >= 0 and y2 + 20 <= frame_height and \n",
    "        x1 - 20 >= 0 and x2 + 20 <= frame_width):\n",
    "        cropped_object = frame[y1-20 :y2 + 20, x1 - 20:x2 + 20]\n",
    "    else:\n",
    "        cropped_object = frame[y1:y2, x1:x2]\n",
    "\n",
    "    prompt0 = \"Question: Is this an animal or a car or a human or a flying-object in the air or a floating-object on the road or an alien? Answer:\"\n",
    "    cropped_image = Image.fromarray(cv2.cvtColor(cropped_object, cv2.COLOR_BGR2RGB))  # Convert to PIL Image\n",
    "    cropped_image = cropped_image.resize((512,512))\n",
    "    inputs = processor_hazard(cropped_image, text=prompt0, return_tensors=\"pt\").to(device, torch.float16)\n",
    "    generated_ids = model_hazard.generate(**inputs, max_new_tokens=10)\n",
    "    # print(f\"Generated IDs: {generated_ids}\")\n",
    "    generated_text_general = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    # print(\"generated_text_general:\", generated_text_general)\n",
    "    generated_text_general = generated_text_general.split()[-1]\n",
    "    # print(\"Last word of generated_text:\", generated_text_general)\n",
    "\n",
    "    contains_car = \"car\" in generated_text_general.lower()\n",
    "    contains_human = any(word in generated_text_general.lower() for word in [\"human\", \"person\", \"man\", \"woman\", \"men\", \"women\", \"kid\"])\n",
    "    contains_animal = any(word in generated_text_general.lower() for word in [\"animal\", \"dog\", \"cat\", \"snake\", \"bird\", \"Kangaroo\", \"moose\", \"deer\", \"rabbit\", \"lizard\", \"cow\", \"horse\", \"goose\", \"duck\", \"mouse\"])\n",
    "    # contains_flyingobject = \"flying-object\" in generated_text_general.lower()\n",
    "    contains_flyingobject = \"air\" in generated_text_general.lower()\n",
    "    contains_object = any(word in generated_text_general.lower() for word in [\"road\", \"alien\"])\n",
    "    \n",
    "    if contains_car:\n",
    "        prompt1 = \"Question: Is this car in the opposing lane or a preceding vehicle or in the wrong way? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_car:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"wrong\", \"opposing\"])\n",
    "        if contains_lane:\n",
    "            is_it_hazard = False\n",
    "\n",
    "    if contains_human:\n",
    "        # Specific prompt to describe appearance\n",
    "        prompt_appearance = \" This person is wearing a\"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_appearance, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_caption = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        prompt1 = \"Question: Is this person crossing the street? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_human:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"yes\"])\n",
    "        \n",
    "        if contains_lane:\n",
    "            caption = str(generated_text_general) + \" The person is going to cross the road \" + appearance_caption\n",
    "            is_it_hazard = True \n",
    "            \n",
    "    if contains_animal:\n",
    "        # Specific prompt to describe appearance\n",
    "        prompt_color = f\" The color of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_color, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_color = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "        prompt_appearance = f\" The characteristic of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_appearance, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_caption = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        prompt1 = \"Question: Is this animal crossing the street? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_animal:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"yes\"])\n",
    "        \n",
    "        if contains_lane:\n",
    "            caption = \"It is a \"+ str(generated_text_general) + f\". The {generated_text_general} is going to cross the road {appearance_color}. {appearance_caption}.\"\n",
    "            is_it_hazard = True  \n",
    "            \n",
    "    if contains_flyingobject:\n",
    "        # Specific prompt to describe appearance\n",
    "        prompt_color = f\" The color of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_color, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_color = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "        prompt_appearance = f\" The characteristic of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_appearance, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_caption = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        prompt1 = \"Question: Is this object thrown into the air? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_flying:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"yes\"])\n",
    "        if contains_lane:\n",
    "            # caption = \"It is a \"+ str(generated_text_general) + f\". The {generated_text_general} is thrown to air {appearance_color}. {appearance_caption}.\"\n",
    "            caption = \"It is a flying-object\" + f\". The flying-object is thrown to air {appearance_color}. {appearance_caption}.\"\n",
    "            is_it_hazard = True\n",
    "\n",
    "    if contains_object:\n",
    "        # Specific prompt to describe appearance\n",
    "        prompt_color = f\" The color of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_color, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_color = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "        prompt_appearance = f\" The characteristic of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_appearance, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_caption = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        prompt1 = \"Question: Is this object on the road? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_object:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"yes\"])\n",
    "        if contains_lane:\n",
    "            caption = \"It is an object on the \"+ str(generated_text_general) + f\". The object is on the road {appearance_color}. {appearance_caption}.\"\n",
    "            is_it_hazard = True\n",
    "        \n",
    "    return is_it_hazard, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38a8e1e1-d72e-40ca-8353-9930de9e3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_caption(captions_list):\n",
    "    \n",
    "    # Step 1: Check for \"is going to\" in the captions\n",
    "    going_to_captions = [item['caption'] for item in captions_list if \"is going to\" in item['caption']]\n",
    "    if going_to_captions:\n",
    "        # print(\"Found 'is going to' in captions:\", going_to_captions)  # Debugging print statement\n",
    "        # Step 2: Find the most repetitive first 4 words if there are multiple \"is going to\" captions\n",
    "        first_four_words = [caption.split()[:4] for caption in going_to_captions]\n",
    "        most_common = Counter([\" \".join(words) for words in first_four_words]).most_common(1)[0][0]\n",
    "        for caption in going_to_captions:\n",
    "            if \" \".join(caption.split()[:4]) == most_common:\n",
    "                return caption\n",
    "\n",
    "    # Step 3: Check for phrases like \"man walking\", \"woman walking\", \"people walking\"\n",
    "    walking_priority = [\"man walking\", \"woman walking\", \"people walking\"]\n",
    "    for priority in walking_priority:\n",
    "        for item in captions_list:\n",
    "            if priority in item['caption']:\n",
    "                return item['caption']\n",
    "\n",
    "    # Step 4: Check for other priority words\n",
    "    other_priority_words = [\"piece of paper\", \"pieces of papers\", \"pieces of paper\", \"box\", \"foggy city\", \"bicycle\", \"ceiling fan\", \"skateboard\"]\n",
    "    for priority_word in other_priority_words:\n",
    "        for item in captions_list:\n",
    "            if priority_word in item['caption']:\n",
    "                return item['caption']\n",
    "\n",
    "    # Step 5: Check for the word \"object\"\n",
    "    for item in captions_list:\n",
    "        if \"object\" in item['caption']:\n",
    "            return item['caption']\n",
    "\n",
    "    # Step 6: Check for \"small bird\" repeated more than 6 times\n",
    "    small_bird_captions = [item['caption'] for item in captions_list if \"small bird\" in item['caption']]\n",
    "    if len(small_bird_captions) > 6:\n",
    "        return small_bird_captions[0]\n",
    "\n",
    "    # Step 7: Random caption as fallback\n",
    "    return random.choice(captions_list)['caption']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb0dc59-dbd4-4a16-bf5c-1a4d48873b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = open(\"./annotations_public.pkl\", 'rb')\n",
    "annotations = pickle.load(annotation_file)#Load annotations\n",
    "annotation_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6fff35-c156-46ad-bb81-3ef2ee768eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_dr = [10, 15, 23, 31, 43, 53, 57, 62, 66, 74, 75, 78, 79, 83, 85, 90, 95, 104, 116, 139, 141, 143, 153, 155, 161, 166, 175,\n",
    "            178, 181, 191]\n",
    "wrong_cap_video = [4, 5, 9, 10, 11, 12, 14, 21, 22, 23, 25, 26, 31, 34, 36, 43, 46, 54, 61, 62, 66, 67, 71, 73, 74, 76, 79, 81, 82, \n",
    "            83, 87, 89, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 108, 109, 111, 112, 116, 121, 122, 123, 126, 127, 128, 129, \n",
    "            130, 132, 134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146, 147, 150, 153, 160, 161, 162, 164, 168, 169, 172, 176,\n",
    "            177, 178, 181, 184, 186, 189, 191, 194, 200]\n",
    "more_obj = [15, 25, 26, 33, 35, 36, 59, 69, 70, 137, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_mine = \"results_md_blip_final_v6_no_nan_dec18.csv\"  \n",
    "df_mine = pd.read_csv(input_file_mine)\n",
    "hazard_dict_mine = {}\n",
    "\n",
    "for index, row in df_mine.iterrows():\n",
    "    video_id = row[\"ID\"]  # Replace with your column name for video ID\n",
    "    # frame_id = row[\"frame\"]  # Replace with your column name for frame ID\n",
    "    match = re.search(r'video_(\\d+)_\\d+', video_id)\n",
    "    video_number = int(match.group(1))\n",
    "\n",
    "    hazard_tracks = []\n",
    "    for i in range(23):  # Assuming Hazard_Track_0 to Hazard_Track_22\n",
    "        hazard_name_col = f'Hazard_Name_{i}'\n",
    "        hazard_track_col = f'Hazard_Track_{i}'\n",
    "        if row[hazard_track_col] == ' ':\n",
    "            continue\n",
    "        hazard_tracks.append(row[hazard_track_col])\n",
    "\n",
    "    # Add to the dictionary using (video_id, frame_id) as the key\n",
    "    hazard_dict_mine[video_id] = hazard_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ee5662-51f6-4c8d-b0e0-f0b53e892af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_dr = \"/home/booster/Downloads/submission_results_Post.csv\"  \n",
    "df_dr = pd.read_csv(input_file_dr)\n",
    "hazard_dict_dr = {}\n",
    "\n",
    "for index, row in df_dr.iterrows():\n",
    "    video_id = row[\"ID\"]  # Replace with your column name for video ID\n",
    "    # frame_id = row[\"frame\"]  # Replace with your column name for frame ID\n",
    "    match = re.search(r'video_(\\d+)_\\d+', video_id)\n",
    "    video_number = int(match.group(1))\n",
    "\n",
    "    hazard_tracks = []\n",
    "    for i in range(22):  # Assuming Hazard_Track_0 to Hazard_Track_22\n",
    "        hazard_name_col = f'Hazard_Name_{i}'\n",
    "        hazard_track_col = f'Hazard_Track_{i}'\n",
    "        if row[hazard_track_col] == ' ':\n",
    "            continue\n",
    "        hazard_tracks.append(row[hazard_track_col])\n",
    "\n",
    "    # Add to the dictionary using (video_id, frame_id) as the key\n",
    "    hazard_dict_dr[video_id] = hazard_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87e5694e-5d9d-40b7-b1f7-73fe176077c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n",
      "23\n",
      "31\n",
      "43\n",
      "53\n",
      "57\n",
      "62\n",
      "66\n",
      "74\n",
      "75\n",
      "76\n",
      "78\n",
      "83\n",
      "85\n",
      "90\n",
      "95\n",
      "104\n",
      "116\n",
      "139\n",
      "141\n",
      "143\n",
      "153\n",
      "155\n",
      "161\n",
      "166\n",
      "175\n",
      "178\n",
      "181\n",
      "191\n"
     ]
    }
   ],
   "source": [
    "for videos in sorted(list(annotations.keys())):\n",
    "    video_id = int(videos.split('_')[-1])\n",
    "    # print(\"video_id:\", video_id)\n",
    "    if video_id in videos_dr:\n",
    "        print(video_id)\n",
    "        for keys in hazard_dict_dr:\n",
    "            keys_split = int(keys.split('_')[1])\n",
    "            # print(\"keys_split:\", keys)\n",
    "            if int(keys_split) in videos_dr:\n",
    "                # print(\"keys2:\", keys_split)\n",
    "                if keys in hazard_dict_mine:\n",
    "                    # print(\"keys3:\",keys)\n",
    "                    hazard = []\n",
    "                    for value in hazard_dict_dr[keys]:\n",
    "                        if value == -1:\n",
    "                            continue\n",
    "                        else:\n",
    "                            hazard.append(str(float(value)))\n",
    "                    hazard_dict_mine[keys] = hazard      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65badef0-705a-4a8b-b0d1-a6cafdac44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_root = './COOOL_Benchmark/processed_videos/'\n",
    "output_dir = \"./COOOL_Benchmark/processed_videos_midas/\"\n",
    "\n",
    "results_file_path = \"results_md_blip_final_v6_.csv\"\n",
    "results_file_path_out = \"results_md_blip_final_v6_new.csv\"\n",
    "\n",
    "directory_path = './captions_output/'\n",
    "\n",
    "# Load existing CSV or create a new DataFrame\n",
    "if os.path.exists(results_file_path):\n",
    "    results_df = pd.read_csv(results_file_path)\n",
    "else:\n",
    "    columns = [\"ID\", \"Driver_State_Changed\"] + [f\"Hazard_Track_{i}\" for i in range(23)] + [f\"Hazard_Name_{i}\" for i in range(23)]\n",
    "    results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "video_num = 0\n",
    "for video in sorted(list(annotations.keys())):\n",
    "    print(video)\n",
    "    video_num += 1\n",
    "    if video_num > 5:\n",
    "        video_stream = cv2.VideoCapture(os.path.join(video_root, video+'.mp4'))\n",
    "        fps = int(video_stream.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(video_stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video_stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for output video\n",
    "        output_video_path = os.path.join(output_dir, f\"{video}_midas_hazard_v10_road.mp4\")\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "        frame = 0\n",
    "        caption_obj = {}\n",
    "        while video_stream.isOpened():\n",
    "            ret, frame_image = video_stream.read()\n",
    "            if ret == False: #False means end of video or error\n",
    "                assert frame == len(annotations[video].keys()) #End of the video must be final frame\n",
    "                break\n",
    "                    \n",
    "            if frame == 0:\n",
    "                frame += 1\n",
    "                continue\n",
    "    \n",
    "            hazard_tracks = []\n",
    "            hazard_captions = []\n",
    "            video_frame_id = f\"{video}_{frame}\"\n",
    "            # print(\"video_frame_id:\", video_frame_id)\n",
    "\n",
    "            if video_frame_id in results_df['ID'].values:  # Check if the ID exists in the DataFrame\n",
    "                row_data = results_df.loc[results_df['ID'] == video_frame_id].iloc[0].to_dict()\n",
    "                # print(f\"Data for {video_frame_id}: {row_data}\")\n",
    "            if video_num in wrong_cap_video:\n",
    "               if len(hazard_dict_mine[video_frame_id]) > 0:\n",
    "                    for track_id in hazard_dict_mine[video_frame_id]:\n",
    "                        hazard_tracks.append(track_id)\n",
    "                        if track_id not in caption_obj:\n",
    "                            print(\"hazard_dict_mine[video_frame_id]:\", hazard_dict_mine[video_frame_id])\n",
    "                            track_id_pkl = int(float(track_id))\n",
    "                            if track_id_pkl not in caption_obj:\n",
    "                                prefix = f\"cap_{video}_{track_id_pkl}\"\n",
    "                                for file_name in os.listdir(directory_path):\n",
    "                                    # Check if the file starts with the desired prefix and ends with '.pkl'\n",
    "                                    if file_name.startswith(prefix) and file_name.endswith('.pkl'):\n",
    "                                        file_path = os.path.join(directory_path, file_name)  # Full path to the file\n",
    "                                        try:\n",
    "                                            # Open and read the .pkl file\n",
    "                                            with open(file_path, 'rb') as file:\n",
    "                                                data = pickle.load(file)  # Load the content of the .pkl file\n",
    "                                                selected_caption = create_caption(data)\n",
    "                                                # hazard_captions.append(selected_caption)\n",
    "                                                caption_obj[track_id] = selected_caption\n",
    "                                                print(\"caption_obj:\", caption_obj)\n",
    "                                                # print(f\"Contents of {file_name}:\")\n",
    "                                        except Exception as e:\n",
    "                                            print(f\"Error reading {file_name}: {e}\")\n",
    "                        else: \n",
    "                            caption = caption_obj[track_id]\n",
    "                        hazard_captions.append(selected_caption)\n",
    "                            \n",
    "            for i in range(min(len(hazard_tracks), 23)):\n",
    "                row_data[f\"Hazard_Track_{i}\"] = hazard_tracks[i]\n",
    "                row_data[f\"Hazard_Name_{i}\"] = hazard_captions[i]\n",
    "    \n",
    "            if video_frame_id in results_df['ID'].values:\n",
    "                    for key, value in row_data.items():\n",
    "                        results_df.loc[results_df['ID'] == video_frame_id, key] = value\n",
    "            else:\n",
    "                results_df = pd.concat([results_df, pd.DataFrame([row_data])], ignore_index=True)\n",
    "            \n",
    "                \n",
    "\n",
    "            frame += 1\n",
    "        results_df.to_csv(results_file_path_out, index=False)\n",
    "        video_stream.release()\n",
    "        out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ac831-6fe0-4577-a0de-17e6c57b7a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Specify the directory containing .pkl files\n",
    "directory_path = './captions_output/'\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith('.pkl'):  # Check if the file has a .pkl extension\n",
    "        file_path = os.path.join(directory_path, file_name)  # Full path to the file\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:  # Open the file in binary read mode\n",
    "                data = pickle.load(file)  # Load the content of the .pkl file\n",
    "                \n",
    "                print(f\"Contents of {file_name} (line by line):\")\n",
    "                if isinstance(data, dict):\n",
    "                    for key, value in data.items():\n",
    "                        print(f\"{key}: {value}\")\n",
    "                elif isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        print(item)\n",
    "                else:\n",
    "                    print(data)  # Print directly if the content is not iterable\n",
    "                print(\"\\n\")  # Add spacing between files\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Contents of cap_video_0140_4_129.pkl (line by line):\n",
    "{'frame': 119, 'caption': 'a close up of a truck with a large piece'}\n",
    "{'frame': 120, 'caption': 'a close up of a truck with a box on'}\n",
    "{'frame': 121, 'caption': 'a close up of a truck with a box on'}\n",
    "{'frame': 122, 'caption': 'a close up of a truck with a green box'}\n",
    "{'frame': 123, 'caption': 'arafed truck on the road with a green box on'}\n",
    "{'frame': 124, 'caption': 'arafed vehicle on the road with a green square on'}\n",
    "{'frame': 125, 'caption': 'a picture taken from a vehicle of a car driving'}\n",
    "{'frame': 126, 'caption': 'a close up of a car driving down a highway'}\n",
    "{'frame': 127, 'caption': 'a close up of a car driving down a highway'}\n",
    "{'frame': 128, 'caption': 'araf truck driving down a highway with a green screen'}\n",
    "{'frame': 129, 'caption': 'there is a man standing in a bathroom with a'}\n",
    "{'frame': 130, 'caption': 'a close up of a car driving down a highway'}\n",
    "{'frame': 131, 'caption': 'arafed view of a car driving down a highway with'}\n",
    "{'frame': 132, 'caption': 'a close up of a car driving down a highway'}\n",
    "{'frame': 133, 'caption': 'a close up of a car driving down a road'}\n",
    "{'frame': 134, 'caption': 'a close up of a car with a broken hood'}\n",
    "{'frame': 135, 'caption': 'a close up of a car driving down a highway'}\n",
    "{'frame': 136, 'caption': 'a picture taken from a car shows a truck that'}\n",
    "{'frame': 137, 'caption': 'a close up of a car driving down a highway'}\n",
    "{'frame': 138, 'caption': 'a close up of a truck driving down a highway'}\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
