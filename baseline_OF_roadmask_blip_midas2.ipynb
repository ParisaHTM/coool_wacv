{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54329b51-f005-4b98-b21f-148b3285c31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/booster/miniconda3/envs/nafnet/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from PIL import Image\n",
    "# from clip_interrogator import Config, Interrogator\n",
    "from matplotlib import pyplot as plt\n",
    "import pdb\n",
    "import subprocess\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ed7054-9cfc-4be2-89f0-c69db22ea89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = open(\"./annotations_public.pkl\", 'rb')\n",
    "annotations = pickle.load(annotation_file)#Load annotations\n",
    "annotation_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15936532-8781-48af-abfd-945ea8ef439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for corner detection \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "video_root = './COOOL_Benchmark/processed_videos/'\n",
    "output_dir = \"./COOOL_Benchmark/processed_videos_midas/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "video_num = 0\n",
    "\n",
    "def detect_hazard(object_cor, frame):\n",
    "    is_it_hazard = False\n",
    "    caption = \"\"\n",
    "    x1, y1, x2, y2 = object_cor\n",
    "    if x1 < 0:\n",
    "       x1 = 0\n",
    "    if x2 < 0:\n",
    "       x2 = 0\n",
    "    if y1 < 0:\n",
    "       y1 = 0\n",
    "    if y2 < 0:\n",
    "       y2 = 0\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "    # Check if the coordinates are within bounds\n",
    "    if (y1 - 20 >= 0 and y2 + 20 <= frame_height and \n",
    "        x1 - 20 >= 0 and x2 + 20 <= frame_width):\n",
    "        cropped_object = frame[y1-20 :y2 + 20, x1 - 20:x2 + 20]\n",
    "    else:\n",
    "        cropped_object = frame[y1:y2, x1:x2]\n",
    "\n",
    "    prompt0 = \"Question: Is this an animal or a car or a human or a flying-object or a floating-object or an alien? Answer:\"\n",
    "    cropped_image = Image.fromarray(cv2.cvtColor(cropped_object, cv2.COLOR_BGR2RGB))  # Convert to PIL Image\n",
    "    \n",
    "    inputs = processor_hazard(cropped_image, text=prompt0, return_tensors=\"pt\").to(device, torch.float16)\n",
    "    generated_ids = model_hazard.generate(**inputs, max_new_tokens=10)\n",
    "    # print(f\"Generated IDs: {generated_ids}\")\n",
    "    generated_text_general = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    # print(\"generated_text_general:\", generated_text_general)\n",
    "    generated_text_general = generated_text_general.split()[-1]\n",
    "    # print(\"Last word of generated_text:\", generated_text_general)\n",
    "\n",
    "    contains_car = \"car\" in generated_text_general.lower()\n",
    "    contains_human = any(word in generated_text_general.lower() for word in [\"human\", \"person\", \"man\", \"woman\", \"men\", \"women\", \"kid\"])\n",
    "    contains_animal = any(word in generated_text_general.lower() for word in [\"animal\", \"dog\", \"cat\", \"snake\", \"bird\", \"Kangaroo\", \"moose\", \"deer\", \"rabbit\", \"lizard\", \"cow\", \"horse\", \"goose\", \"duck\", \"mouse\"])\n",
    "    contains_flyingobject = \"flying-object\" in generated_text_general.lower()\n",
    "    contains_object = any(word in generated_text_general.lower() for word in [\"road\", \"alien\"])\n",
    "    \n",
    "    if contains_car:\n",
    "        prompt1 = \"Question: Is this car in the opposing lane or a preceding vehicle or in the wrong way? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_car:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"wrong\", \"opposing\"])\n",
    "        if contains_lane:\n",
    "            is_it_hazard = False\n",
    "\n",
    "    if contains_human:\n",
    "        # Specific prompt to describe appearance\n",
    "        prompt_appearance = \" This person is wearing a\"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_appearance, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_caption = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        prompt1 = \"Question: Is this person crossing the street? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_human:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"yes\"])\n",
    "        \n",
    "        if contains_lane:\n",
    "            caption = str(generated_text_general) + \" The person is going to cross the road \" + appearance_caption\n",
    "            is_it_hazard = True \n",
    "            \n",
    "    if contains_animal:\n",
    "        # Specific prompt to describe appearance\n",
    "        prompt_color = f\" The color of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_color, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_color = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "        prompt_appearance = f\" The characteristic of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_appearance, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_caption = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        prompt1 = \"Question: Is this animal crossing the street? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_animal:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"yes\"])\n",
    "        \n",
    "        if contains_lane:\n",
    "            caption = \"It is a \"+ str(generated_text_general) + f\". The {generated_text_general} is going to cross the road {appearance_color}. {appearance_caption}.\"\n",
    "            is_it_hazard = True  \n",
    "            \n",
    "    if contains_flyingobject:\n",
    "        # Specific prompt to describe appearance\n",
    "        prompt_color = f\" The color of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_color, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_color = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "        prompt_appearance = f\" The characteristic of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_appearance, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_caption = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        prompt1 = \"Question: Is this object thrown into the air? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_flying:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"yes\"])\n",
    "        if contains_lane:\n",
    "            caption = \"It is a \"+ str(generated_text_general) + f\". The {generated_text_general} is thrown to air {appearance_color}. {appearance_caption}.\"\n",
    "            is_it_hazard = True\n",
    "\n",
    "    if contains_object:\n",
    "        # Specific prompt to describe appearance\n",
    "        prompt_color = f\" The color of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_color, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_color = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "        prompt_appearance = f\" The characteristic of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_appearance, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_caption = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        prompt1 = \"Question: Is this object on the road? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_object:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"yes\"])\n",
    "        if contains_lane:\n",
    "            caption = \"It is an object on the \"+ str(generated_text_general) + f\". The object is on the road {appearance_color}. {appearance_caption}.\"\n",
    "            is_it_hazard = True\n",
    "        \n",
    "    return is_it_hazard, caption\n",
    "    \n",
    "\n",
    "def analyze_hazard_results(hazard_results):\n",
    "    object_summary = {}\n",
    "\n",
    "    for obj_id, captions in hazard_results.items():\n",
    "        # Extract the first four words of each caption\n",
    "        first_four_words = [\" \".join(caption.split()[:4]) for caption in captions]\n",
    "        \n",
    "        # Count the occurrences of each phrase\n",
    "        word_counts = Counter(first_four_words)\n",
    "        \n",
    "        # Find the most repetitive phrase\n",
    "        most_repetitive_phrase = word_counts.most_common(1)[0][0]\n",
    "        \n",
    "        # Find the caption with the highest length for the most repetitive phrase\n",
    "        filtered_captions = [caption for caption in captions if most_repetitive_phrase in caption]\n",
    "        longest_caption = max(filtered_captions, key=len)\n",
    "        \n",
    "        # Save results for this object\n",
    "        object_summary[obj_id] = {\n",
    "            \"most_repetitive_phrase\": most_repetitive_phrase,\n",
    "            \"count\": word_counts[most_repetitive_phrase],\n",
    "            \"longest_caption\": longest_caption\n",
    "        }\n",
    "    \n",
    "    return object_summary\n",
    "\n",
    "\n",
    "with open(\"results_blip_lp2.csv\", 'w') as results_file:\n",
    "    results_file.write(\"ID,Driver_State_Changed\")\n",
    "    for i in range(23):\n",
    "        results_file.write(f\",Hazard_Track_{i},Hazard_Name_{i}\")\n",
    "    results_file.write(\"\\n\")\n",
    "\n",
    "    processor_hazard = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "    model_hazard = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16).to(device)\n",
    "    \n",
    "    \n",
    "    for video in sorted(list(annotations.keys())):\n",
    "        print(\"video:\", video)\n",
    "        video_num += 1   \n",
    "        if video_num > 0:\n",
    "            # print(\"video_num:\", video_num)\n",
    "            video_stream = cv2.VideoCapture(os.path.join(video_root, video+'.mp4'))\n",
    "            frame = 0\n",
    "            previous_centroids = []\n",
    "            captioned_tracks = {}\n",
    "            track_id_lifecycle = {} \n",
    "            hazard_results = {}\n",
    "    \n",
    "            fps = int(video_stream.get(cv2.CAP_PROP_FPS))\n",
    "            width = int(video_stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(video_stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for output video\n",
    "            output_video_path = os.path.join(output_dir, f\"{video}_midas_hazard_v8.mp4\")\n",
    "            out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "            \n",
    "            frame = 0\n",
    "            video_stream = cv2.VideoCapture(os.path.join(video_root, video+'.mp4'))\n",
    "            while video_stream.isOpened():\n",
    "                \n",
    "                #########################################################################################\n",
    "                # Find objects in closer positions to dashcam using results out of MiDas\n",
    "                video_frame = f'{video}_{frame}'\n",
    "                file_path = f\"./unique_ids/unique_ids_{video}.pkl\"\n",
    "                \n",
    "                with open(file_path, \"rb\") as f:\n",
    "                    unique_ids = pickle.load(f)\n",
    "                ret, frame_image = video_stream.read()\n",
    "                if ret == False: \n",
    "                    # assert frame == len(annotations[video].keys())\n",
    "                    break\n",
    "                #########################################################################################             \n",
    "                #Gather BBoxes from annotations\n",
    "                bboxes = {}\n",
    "                centroids = []\n",
    "                chips = {}\n",
    "                track_ids = []\n",
    "                det_far = {}\n",
    "                for ann_type in ['challenge_object']:\n",
    "                    for i in range(len(annotations[video][frame][ann_type])):\n",
    "                        x1, y1, x2, y2 = annotations[video][frame][ann_type][i]['bbox']\n",
    "                        track_id = annotations[video][frame][ann_type][i]['track_id']\n",
    "                        if track_id in unique_ids:               \n",
    "                            hazard_track = track_id\n",
    "                            # print(\"hazard_track:\", hazard_track)\n",
    "                            object_cor = int(x1), int(y1), int(x2), int(y2)\n",
    "                            ##########################################################\n",
    "                            # Produce captions for each objects and filter them if they are cars\n",
    "                            is_hazard, caption = detect_hazard(object_cor, frame_image)\n",
    "                            if len(caption) > 1:\n",
    "                                # print(\"caption:\", caption)\n",
    "                                if track_id not in hazard_results:\n",
    "                                    hazard_results[track_id] = []\n",
    "                                hazard_results[track_id].append(caption)\n",
    "                            # Draw bounding box and object ID on the frame\n",
    "                            if is_hazard:\n",
    "                                cv2.rectangle(frame_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                                cv2.putText(frame_image, f\"ID: {hazard_track}\", (int(x1), int(y1) - 10),\n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "                                cv2.putText(frame_image, str(is_hazard)[0], (int(x2), int(y2) + 10),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "                                cv2.putText(frame_image, caption, (int(x2), int(y2) + 20),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "                    \n",
    "                out.write(frame_image)\n",
    "                        \n",
    "                frame +=1\n",
    "            # print(\"filtered_summary:\", filtered_summary, \"\\n\")\n",
    "            video_stream.release()\n",
    "            # print(analyze_hazard_results(hazard_results)) \n",
    "            object_summary = analyze_hazard_results(hazard_results)\n",
    "            output_file_path = f\"hazard_results/hazard_results_{video}_v8.pkl\"\n",
    "            with open(output_file_path, \"wb\") as pkl_file:\n",
    "                pickle.dump(object_summary, pkl_file)\n",
    "\n",
    "        # if video_num == 9:\n",
    "        #     break\n",
    "        out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c9a5b5-8753-4ebe-9e5b-336e6890db74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1001': 354,\n",
       " '431': 324,\n",
       " '408': 108,\n",
       " '3': 361,\n",
       " '6': 0,\n",
       " '5': 6,\n",
       " '73': 265,\n",
       " '79': 89}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in previous step we saved the if of each objects with their frame in which they have brightest color (meaning they are so close to dashcam in that specific frame. \n",
    "# and we will produce captions based in these frames for each objects)\n",
    "file_path = f\"./unique_ids/unique_ids_video_0001.pkl\"\n",
    "# Open the pickle file and load its contents\n",
    "with open(file_path, \"rb\") as f:\n",
    "    unique_ids = pickle.load(f)\n",
    "unique_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nafnet",
   "language": "python",
   "name": "nafnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
