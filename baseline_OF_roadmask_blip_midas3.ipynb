{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2ab19b-8869-4473-90bd-a3498141d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "from clip_interrogator import Config, Interrogator\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d707e885-6077-4e30-89a1-a516cfd8efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = open(\"./annotations_public.pkl\", 'rb')\n",
    "annotations = pickle.load(annotation_file)#Load annotations\n",
    "annotation_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840d90e-7685-4ab0-89b7-2d8e7dc7ce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"road_objects.csv\")\n",
    "\n",
    "# Group by 'video' and collect track_ids into a list\n",
    "result = df.groupby('video')['track_id'].apply(list).to_dict()\n",
    "\n",
    "for video in result:\n",
    "    print(f\"{video}: {result[video]}\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68eca4aa-4e45-4b9f-8e84-e86c43ccd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hazard(object_cor, frame):\n",
    "    is_it_hazard = False\n",
    "    caption = \"\"\n",
    "    x1, y1, x2, y2 = object_cor\n",
    "    if x1 < 0:\n",
    "       x1 = 0\n",
    "    if x2 < 0:\n",
    "       x2 = 0\n",
    "    if y1 < 0:\n",
    "       y1 = 0\n",
    "    if y2 < 0:\n",
    "       y2 = 0\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "    # Check if the coordinates are within bounds\n",
    "    if (y1 - 20 >= 0 and y2 + 20 <= frame_height and \n",
    "        x1 - 20 >= 0 and x2 + 20 <= frame_width):\n",
    "        cropped_object = frame[y1-20 :y2 + 20, x1 - 20:x2 + 20]\n",
    "    else:\n",
    "        cropped_object = frame[y1:y2, x1:x2]\n",
    "\n",
    "    prompt0 = \"Question: Is this an animal or a car or a human or a flying-object or an floating-object on the road or an alien? Answer:\"\n",
    "    cropped_image = Image.fromarray(cv2.cvtColor(cropped_object, cv2.COLOR_BGR2RGB))  # Convert to PIL Image\n",
    "    cropped_image = cropped_image.resize((512,512))\n",
    "    inputs = processor_hazard(cropped_image, text=prompt0, return_tensors=\"pt\").to(device, torch.float16)\n",
    "    generated_ids = model_hazard.generate(**inputs, max_new_tokens=10)\n",
    "    # print(f\"Generated IDs: {generated_ids}\")\n",
    "    generated_text_general = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    # print(\"generated_text_general:\", generated_text_general)\n",
    "    generated_text_general = generated_text_general.split()[-1]\n",
    "    # print(\"Last word of generated_text:\", generated_text_general)\n",
    "\n",
    "    contains_car = \"car\" in generated_text_general.lower()\n",
    "    contains_human = any(word in generated_text_general.lower() for word in [\"human\", \"person\", \"man\", \"woman\", \"men\", \"women\", \"kid\"])\n",
    "    contains_animal = any(word in generated_text_general.lower() for word in [\"animal\", \"dog\", \"cat\", \"snake\", \"bird\", \"Kangaroo\", \"moose\", \"deer\", \"rabbit\", \"lizard\", \"cow\", \"horse\", \"goose\", \"duck\", \"mouse\"])\n",
    "    contains_flyingobject = \"flying-object\" in generated_text_general.lower()\n",
    "    contains_object = any(word in generated_text_general.lower() for word in [\"road\", \"alien\"])\n",
    "    \n",
    "    if contains_car:\n",
    "        prompt1 = \"Question: Is this car in the opposing lane or a preceding vehicle or in the wrong way? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_car:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"wrong\", \"opposing\"])\n",
    "        if contains_lane:\n",
    "            is_it_hazard = False\n",
    "\n",
    "    if contains_human:\n",
    "        # Specific prompt to describe appearance\n",
    "        prompt_appearance = \" This person is wearing a\"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_appearance, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_caption = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        prompt1 = \"Question: Is this person crossing the street? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_human:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"yes\"])\n",
    "        \n",
    "        if contains_lane:\n",
    "            caption = str(generated_text_general) + \" The person is going to cross the road \" + appearance_caption\n",
    "            is_it_hazard = True \n",
    "            \n",
    "    if contains_animal:\n",
    "        # Specific prompt to describe appearance\n",
    "        prompt_color = f\" The color of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_color, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_color = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "        prompt_appearance = f\" The characteristic of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_appearance, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_caption = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        prompt1 = \"Question: Is this animal crossing the street? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_animal:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"yes\"])\n",
    "        \n",
    "        if contains_lane:\n",
    "            caption = \"It is a \"+ str(generated_text_general) + f\". The {generated_text_general} is going to cross the road {appearance_color}. {appearance_caption}.\"\n",
    "            is_it_hazard = True  \n",
    "            \n",
    "    if contains_flyingobject:\n",
    "        # Specific prompt to describe appearance\n",
    "        prompt_color = f\" The color of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_color, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_color = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "        prompt_appearance = f\" The characteristic of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_appearance, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_caption = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        prompt1 = \"Question: Is this object thrown into the air? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_flying:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"yes\"])\n",
    "        if contains_lane:\n",
    "            caption = \"It is a \"+ str(generated_text_general) + f\". The {generated_text_general} is thrown to air {appearance_color}. {appearance_caption}.\"\n",
    "            is_it_hazard = True\n",
    "\n",
    "    if contains_object:\n",
    "        # Specific prompt to describe appearance\n",
    "        prompt_color = f\" The color of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_color, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_color = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "        prompt_appearance = f\" The characteristic of the {generated_text_general} \"\n",
    "        inputs = processor_hazard(cropped_image, text=prompt_appearance, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=20)  # Limit the response to approximately 10 words\n",
    "        appearance_caption = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        prompt1 = \"Question: Is this object on the road? Answer:\"\n",
    "        # print(\"prompt1:\", prompt1)\n",
    "        inputs = processor_hazard(cropped_image, text=prompt1, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model_hazard.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = processor_hazard.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        # print(\"generated_text_object:\", generated_text)\n",
    "        contains_lane = any(word in generated_text.lower() for word in [\"yes\"])\n",
    "        if contains_lane:\n",
    "            caption = \"It is an object on the \"+ str(generated_text_general) + f\". The object is on the road {appearance_color}. {appearance_caption}.\"\n",
    "            is_it_hazard = True\n",
    "        \n",
    "    return is_it_hazard, caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7e2a76a-c150-4027-8659-86ad8da16ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_track_ids(\n",
    "    caption_midas_ids, \n",
    "    forbidden_word_count, \n",
    "    hazard_results, \n",
    "    captioned_tracks, \n",
    "    frame_image, \n",
    "    ci, \n",
    "    x1, y1, x2, y2,\n",
    "):\n",
    "\n",
    "    # Case 1: Caption already exists in caption_midas_ids (from hazard_results_{video}_v3.pkl:contains caption from most frequent caption that were produced in last step and save the longest caption)\n",
    "    if track_id in caption_midas_ids:\n",
    "        # print(\"track_id2:\", track_id)\n",
    "        caption_to_display = caption_midas_ids[track_id]['longest_caption']\n",
    "        hazard_tracks.append(track_id)\n",
    "        hazard_captions.append(caption_to_display)\n",
    "        \n",
    "        # Draw bounding box and caption\n",
    "        cv2.rectangle(frame_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        display_text = f\"ID: {track_id} | {caption_to_display}\"\n",
    "        cv2.putText(frame_image, display_text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (255, 0, 0), 2)\n",
    "            \n",
    "    else:\n",
    "        if track_id not in captioned_tracks:\n",
    "            print(\"track_id3:\", track_id)   \n",
    "            # Case 2: Generate caption if not in caption_midas_ids\n",
    "            object_cor = (int(x1), int(y1), int(x2), int(y2))\n",
    "            is_hazard, caption = detect_hazard(object_cor, frame_image)\n",
    "    \n",
    "            if len(caption) > 1:\n",
    "                # Successful caption generation\n",
    "                hazard_results.setdefault(track_id, []).append(caption)\n",
    "                hazard_tracks.append(track_id)\n",
    "                hazard_captions.append(caption)\n",
    "                captioned_tracks[track_id] = caption\n",
    "    \n",
    "                # Draw bounding box and caption\n",
    "                cv2.rectangle(frame_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                display_text = f\"ID: {track_id} | {caption}\"\n",
    "                cv2.putText(frame_image, display_text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.5, (255, 0, 0), 2)\n",
    "            else:\n",
    "                # Use VIT-B-L again for caption\n",
    "                hazard_tracks.append(track_id)\n",
    "                forbidden_word_count[track_id] = forbidden_word_count.get(track_id, 0)\n",
    "\n",
    "                chip = frame_image[int(y1):int(y2), int(x1):int(x2)]\n",
    "                hazard_chip = cv2.cvtColor(chip, cv2.COLOR_BGR2RGB)\n",
    "                hazard_chip = Image.fromarray(hazard_chip)\n",
    "                hazard_chip = hazard_chip.resize((512, 512))\n",
    "\n",
    "                # Generate caption\n",
    "                caption = ci.interrogate(hazard_chip)\n",
    "                caption1 = caption.replace(\",\", \" \")\n",
    "                caption = \" \".join(caption1.split()[:10])\n",
    "\n",
    "                # Check for forbidden words\n",
    "                if \"car\" in caption1.lower() or \"vehicle\" in caption1.lower():\n",
    "                    forbidden_word_count[track_id] += 1\n",
    "                    hazard_tracks.remove(track_id)  # Remove invalid track\n",
    "                else:\n",
    "                    captioned_tracks[track_id] = caption\n",
    "                    hazard_captions.append(caption)\n",
    "\n",
    "                    # Draw bounding box and caption\n",
    "                    cv2.rectangle(frame_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                    display_text = f\"ID: {track_id} | {caption}\"\n",
    "                    cv2.putText(frame_image, display_text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                0.5, (255, 0, 0), 2)\n",
    "        else:\n",
    "            # print(\"track_id4:\", track_id)\n",
    "            hazard_tracks.append(track_id)\n",
    "            caption = captioned_tracks[track_id]\n",
    "            hazard_captions.append(caption)\n",
    "\n",
    "        # Draw bounding box and caption\n",
    "        cv2.rectangle(frame_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        display_text = f\"ID: {track_id} | {caption}\"\n",
    "        cv2.putText(frame_image, display_text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (255, 0, 0), 2)\n",
    "\n",
    "    return hazard_tracks, hazard_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d3f5a-63a1-429f-bf17-3edf928845f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "# File paths\n",
    "results_file_path = \"results_md_blip_final_v6.csv\"\n",
    "results_file_path_out = \"results_md_blip_final_v6.csv\"\n",
    "video_root = './COOOL_Benchmark/processed_videos/'\n",
    "output_dir = \"./COOOL_Benchmark/processed_videos_midas/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "ann_type = 'challenge_object'\n",
    "ci = Interrogator(Config(clip_model_name=\"ViT-B-32/openai\"))\n",
    "\n",
    "video_num = 0\n",
    "\n",
    "# Load existing CSV or create a new DataFrame\n",
    "if os.path.exists(results_file_path):\n",
    "    results_df = pd.read_csv(results_file_path)\n",
    "else:\n",
    "    columns = [\"ID\", \"Driver_State_Changed\"] + [f\"Hazard_Track_{i}\" for i in range(23)] + [f\"Hazard_Name_{i}\" for i in range(23)]\n",
    "    results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Iterate through videos\n",
    "for video in sorted(list(annotations.keys())):  # Iterate through available videos\n",
    "    print(\"video:\", video)\n",
    "    video_num += 1\n",
    "    captioned_tracks = {}\n",
    "    forbidden_word_count = {}\n",
    "    if video_num > 0:\n",
    "        if video in result:\n",
    "            road_mask_result = result[video]  # int\n",
    "        else:\n",
    "            road_mask_result = {}            \n",
    "        road_mask_str = [str(val) for val in road_mask_result] #str\n",
    "\n",
    "        \n",
    "        id_caption_v3 = f\"hazard_results/hazard_results_{video}_v3.pkl\"\n",
    "        if os.path.exists(id_caption_v3):\n",
    "            with open(id_caption_v3, \"rb\") as f:\n",
    "                caption_midas_ids = pickle.load(f)  #str\n",
    "        else:\n",
    "            caption_midas_ids = {}\n",
    "\n",
    "        \n",
    "        midas = f\"unique_ids/unique_ids_{video}.pkl\"\n",
    "        if os.path.exists(midas):\n",
    "            with open(midas, \"rb\") as f:\n",
    "                midas_ids = pickle.load(f)  #str\n",
    "        else:\n",
    "            midas_ids = {}\n",
    "\n",
    "        mutual_values = [key for key in road_mask_str if key in midas_ids]\n",
    "        # print(\"mutual_values:\", mutual_values)\n",
    "\n",
    "        captions = {}\n",
    "        for value in mutual_values:\n",
    "            if value in caption_midas_ids:\n",
    "                captions[value] = caption_midas_ids[value]['longest_caption']\n",
    "        # print(\"len(captions):\", len(captions))\n",
    "\n",
    "        video_stream = cv2.VideoCapture(os.path.join(video_root, video+'.mp4'))\n",
    "        fps = int(video_stream.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(video_stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video_stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for output video\n",
    "        output_video_path = os.path.join(output_dir, f\"{video}_midas_hazard_v7_road.mp4\")\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "        processor_hazard = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "        model_hazard = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "        frame = 0\n",
    "        captioned_tracks = {}\n",
    "        hazard_results = {}\n",
    "        \n",
    "        while video_stream.isOpened():\n",
    "        \n",
    "            ret, frame_image = video_stream.read()\n",
    "            if ret == False: #False means end of video or error\n",
    "                assert frame == len(annotations[video].keys()) #End of the video must be final frame\n",
    "                break\n",
    "                \n",
    "            if frame == 0:\n",
    "                frame += 1\n",
    "                continue\n",
    "          \n",
    "            hazard_tracks = []\n",
    "            hazard_captions = []\n",
    "            \n",
    "            video_frame_id = f\"{video}_{frame}\"\n",
    "            # print(video_frame_id)\n",
    "            driver_state_flag = results_df.loc[results_df['ID'] == video_frame_id, 'Driver_State_Changed'].values[0]  # Replace this with your actual logic\n",
    "            row_data = {\"ID\": video_frame_id, \"Driver_State_Changed\": driver_state_flag}\n",
    "            for i in range(23):\n",
    "                row_data[f\"Hazard_Track_{i}\"] = \"\"\n",
    "                row_data[f\"Hazard_Name_{i}\"] = \"\"\n",
    "            \n",
    "            for i in range(len(annotations[video][frame][ann_type])):\n",
    "                x1, y1, x2, y2 = annotations[video][frame][ann_type][i]['bbox']\n",
    "                track_id = annotations[video][frame][ann_type][i]['track_id']\n",
    "\n",
    "                ######### Case 1) caption from midas when there are mutual objects from road mask and MiDas\n",
    "                if 0 < len(captions) <= 6:\n",
    "                   if track_id in captions:\n",
    "                       # print(\"track_id:\", track_id)\n",
    "                       hazard_tracks.append(track_id)\n",
    "                       hazard_captions.append(captions[track_id])\n",
    "                       cv2.rectangle(frame_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                       display_text = f\"ID: {track_id} | {captions[track_id]}\"                            \n",
    "                       cv2.putText(frame_image, display_text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                        0.5, (255, 0, 0), 2)\n",
    "                       \n",
    "                ######### Case 6) when more than 10 hazards and midas cannot find them --> objects from road mask and createing caption for them\n",
    "                if len(road_mask_str) > 12 and len(captions) < 7:\n",
    "                    combined_ids_road = list(set(road_mask_str) | set(midas_ids.keys()))\n",
    "                    print(\"method 6\")\n",
    "                    if track_id in forbidden_word_count:\n",
    "                        if forbidden_word_count[track_id] >= 10:\n",
    "                            continue\n",
    "                    if track_id in combined_ids_road:   \n",
    "                        if caption_midas_ids:\n",
    "                          caption_create = False\n",
    "                          if caption_midas_ids and caption_create == False:\n",
    "                              caption_create = True\n",
    "                              first_key = list(caption_midas_ids.keys())[0]\n",
    "                              caption_0 = caption_midas_ids[first_key]['longest_caption']\n",
    "                          hazard_tracks.append(track_id)\n",
    "                          hazard_captions.append(caption_0)\n",
    "                        else:                           \n",
    "                          hazard_tracks, hazard_captions = process_track_ids(\n",
    "                                                            caption_midas_ids, \n",
    "                                                            forbidden_word_count, \n",
    "                                                            hazard_results, \n",
    "                                                            captioned_tracks, \n",
    "                                                            frame_image, \n",
    "                                                            ci, \n",
    "                                                            x1, y1, x2, y2\n",
    "                                                        )\n",
    "                              \n",
    "                                           \n",
    "                ######## Case 5) vidoes with more than 10 hazard ---> combinaiton of all objects from MiDas and road masking\n",
    "                if len(captions) >= 7:   # 15; Goose consider both road and midas caption\n",
    "                  print(\"method 5\")\n",
    "                  combined_ids = list(set(road_mask_str) | set(caption_midas_ids.keys()))\n",
    "                  caption_create = False\n",
    "                  if caption_midas_ids and caption_create == False:\n",
    "                      caption_create = True\n",
    "                      first_key = list(caption_midas_ids.keys())[0]\n",
    "                      caption_0 = caption_midas_ids[first_key]['longest_caption']\n",
    "                  hazard_tracks.append(track_id)\n",
    "                  hazard_captions.append(caption_0)\n",
    "                # Caption video without caption\n",
    "                # Try to caption with blip agian\n",
    "                if len(captions) == 0:\n",
    "                    #Case 3) when no objects from road mask but there is some form MiDas\n",
    "                    if len(caption_midas_ids) > 0:  #Case 5) when no road but caption midas: caption was produces in previous step\n",
    "                        if track_id in caption_midas_ids:\n",
    "                           # print(\"track_id:\", track_id)\n",
    "                           hazard_tracks.append(track_id)\n",
    "                           caption_to_display = caption_midas_ids[track_id]['longest_caption']\n",
    "                           hazard_captions.append(caption_to_display)\n",
    "\n",
    "                           cv2.rectangle(frame_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                           display_text = f\"ID: {track_id} | {caption_to_display}\"                            \n",
    "                           cv2.putText(frame_image, display_text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                        0.5, (255, 0, 0), 2)\n",
    " \n",
    "                    if len(caption_midas_ids) == 0:\n",
    "                        #Case 6) if object fram road mask and MiDas is there but no caption was produces in last steps for none of them\n",
    "                        if mutual_values:\n",
    "                            if track_id in mutual_values:  #4 both road and midas same but no caption midas\n",
    "                              # print(\"mutual_values:\", mutual_values)\n",
    "                              if track_id in forbidden_word_count:\n",
    "                                  if forbidden_word_count[track_id] >= 10:\n",
    "                                    continue\n",
    "                              object_cor = int(x1), int(y1), int(x2), int(y2)                           \n",
    "                              is_hazard, caption = detect_hazard(object_cor, frame_image)\n",
    "                              # print(\"track_id:\", track_id)\n",
    "            \n",
    "                              ###### Case 2) If blip produces any caption\n",
    "                              if len(caption) > 1:\n",
    "                                # print(\"caption:\", caption)\n",
    "                                if track_id not in hazard_results:\n",
    "                                    hazard_results[track_id] = []\n",
    "                                hazard_results[track_id].append(caption)\n",
    "                                hazard_tracks.append(track_id)\n",
    "                                hazard_captions.append(caption)\n",
    "    \n",
    "                                cv2.rectangle(frame_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                                display_text = f\"ID: {track_id} | {caption}\"                            \n",
    "                                cv2.putText(frame_image, display_text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                            0.5, (255, 0, 0), 2)\n",
    "        \n",
    "                              ###########################################################################################################\n",
    "                              #### If blip cannot produce any caption: we get help from VIT-B-L again with bbox :(\n",
    "                              if len(caption) == 0:\n",
    "                                  hazard_tracks.append(track_id)\n",
    "                                  if track_id not in captioned_tracks:\n",
    "                                    chip = frame_image[int(y1):int(y2), int(x1):int(x2)]\n",
    "                                    hazard_chip = cv2.cvtColor(chip, cv2.COLOR_BGR2RGB)\n",
    "                                    hazard_chip = Image.fromarray(hazard_chip)\n",
    "                                    hazard_chip = hazard_chip.resize((512, 512))\n",
    "                                    # Generate caption\n",
    "                                    caption = ci.interrogate(hazard_chip)\n",
    "                        \n",
    "                                    caption1 = caption.replace(\",\",\" \")\n",
    "                                    caption = \" \".join(caption1.split()[:10])\n",
    "                                    # print(\"caption;\", caption)\n",
    "                                    \n",
    "                                    captioned_tracks[track_id] = caption\n",
    "                                  else:\n",
    "                                    caption = captioned_tracks[track_id]\n",
    "                                  hazard_captions.append(caption)  \n",
    "    \n",
    "                                  cv2.rectangle(frame_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                                  display_text = f\"ID: {track_id} | {caption}\"                            \n",
    "                                  cv2.putText(frame_image, display_text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                                0.5, (255, 0, 0), 2)\n",
    "                        #2\n",
    "                        else:\n",
    "                          if track_id in midas_ids:\n",
    "                              # print(\"midas_ids:\", midas_ids)\n",
    "                              if track_id in forbidden_word_count:\n",
    "                                  if forbidden_word_count[track_id] >= 10:\n",
    "                                    continue\n",
    "                              #6\n",
    "                              object_cor = int(x1), int(y1), int(x2), int(y2)                           \n",
    "                              is_hazard, caption = detect_hazard(object_cor, frame_image)\n",
    "                              # print(\"track_id:\", track_id)\n",
    "            \n",
    "                              ###### 2 If blip produces any caption: YAY!\n",
    "                              if len(caption) > 1:\n",
    "                                # print(\"caption:\", caption)\n",
    "                                if track_id not in hazard_results:\n",
    "                                    hazard_results[track_id] = []\n",
    "                                hazard_results[track_id].append(caption)\n",
    "                                hazard_tracks.append(track_id)\n",
    "                                hazard_captions.append(caption)\n",
    "    \n",
    "                                cv2.rectangle(frame_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                                display_text = f\"ID: {track_id} | {caption}\"                            \n",
    "                                cv2.putText(frame_image, display_text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                            0.5, (255, 0, 0), 2)\n",
    "        \n",
    "                              ###########################################################################################################\n",
    "                              #### If blip cannot produce any caption: we get help from VIT-B-L again with bbox :(\n",
    "                              if len(caption) == 0:\n",
    "                                  hazard_tracks.append(track_id)\n",
    "                                  if track_id not in captioned_tracks:\n",
    "                                    chip = frame_image[int(y1):int(y2), int(x1):int(x2)]\n",
    "                                    hazard_chip = cv2.cvtColor(chip, cv2.COLOR_BGR2RGB)\n",
    "                                    hazard_chip = Image.fromarray(hazard_chip)\n",
    "                                    hazard_chip = hazard_chip.resize((512, 512))\n",
    "                                    # Generate caption\n",
    "                                    caption = ci.interrogate(hazard_chip)\n",
    "                        \n",
    "                                    caption1 = caption.replace(\",\",\" \")\n",
    "                                    caption = \" \".join(caption1.split()[:10])\n",
    "                                    # print(\"caption;\", caption)\n",
    "                                    \n",
    "                                    captioned_tracks[track_id] = caption\n",
    "                                  else:\n",
    "                                    caption = captioned_tracks[track_id]\n",
    "                                  hazard_captions.append(caption)  \n",
    "    \n",
    "                                  cv2.rectangle(frame_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                                  display_text = f\"ID: {track_id} | {caption}\"                            \n",
    "                                  cv2.putText(frame_image, display_text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                                0.5, (255, 0, 0), 2)\n",
    "                    \n",
    "                    \n",
    "            ####write csv file\n",
    "            for i in range(min(len(hazard_tracks), 23)):\n",
    "                row_data[f\"Hazard_Track_{i}\"] = hazard_tracks[i]\n",
    "                # print(\"row_data[f'Hazard_Track_{i}']\", row_data[f\"Hazard_Track_{i}\"])\n",
    "                row_data[f\"Hazard_Name_{i}\"] = hazard_captions[i]\n",
    "                # print(\" row_data[f'Hazard_Name_{i}']\",  row_data[f\"Hazard_Name_{i}\"])\n",
    "\n",
    "             # Update DataFrame\n",
    "            if video_frame_id in results_df['ID'].values:\n",
    "                for key, value in row_data.items():\n",
    "                    results_df.loc[results_df['ID'] == video_frame_id, key] = value\n",
    "            else:\n",
    "                results_df = pd.concat([results_df, pd.DataFrame([row_data])], ignore_index=True)\n",
    "                      \n",
    "\n",
    "            frame += 1\n",
    "            out.write(frame_image)\n",
    "        # Save updated DataFrame to CSV\n",
    "        results_df.to_csv(results_file_path_out, index=False)\n",
    "        print(\"Results file updated successfully!\")\n",
    "        video_stream.release()\n",
    "        out.release()\n",
    "\n",
    "\n",
    "# Save updated DataFrame to CSV\n",
    "results_df.to_csv(results_file_path_out, index=False)\n",
    "print(\"Results file updated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
